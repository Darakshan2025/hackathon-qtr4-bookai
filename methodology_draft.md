### 3. Methodology

### 3.1. Research Design

This research employs a systematic literature review methodology complemented by a conceptual analysis to investigate agentic architectures within AI-native software development. This dual approach allows for a comprehensive synthesis of existing peer-reviewed knowledge while also enabling the critical examination and theoretical framing of emerging concepts and challenges. The systematic review component ensures rigor and replicability by adhering to a predefined protocol for source identification, selection, and data extraction. The conceptual analysis extends this by interpreting findings, identifying underlying patterns, and constructing a coherent understanding of the architectural implications and future directions of agentic systems in software engineering.

### 3.2. Data Collection and Source Selection

The data collection process focused on identifying primary and peer-reviewed sources from reputable academic databases and digital libraries, including ACM Digital Library, IEEE Xplore, and arXiv. The initial search utilized keywords such as "AI-native software development," "agentic architectures," "LLM-augmented development," and "autonomous coding systems." Strict inclusion criteria were applied: sources had to be peer-reviewed articles, conference papers, or authoritative survey papers published between 2020 and 2025 to ensure currency and relevance to the rapidly evolving field. Grey literature was considered only for contemporary tools and frameworks where formal academic publications are nascent, adhering to a maximum of 25% of total sources as per the project constitution. A curated list of at least 15 scholarly sources, with a minimum of 50% peer-reviewed articles, formed the foundation of the evidence base (see Annotated Bibliography in `specs/Research Paper on AI-Native Software Development/annotated_bibliography.md`).

### 3.3. Data Analysis Approach

The analysis of the collected literature proceeded in several stages. Firstly, a thematic analysis was conducted to identify recurring concepts, definitions, architectural patterns, and challenges related to agentic AI in software development. Each selected source was thoroughly read, and key claims, definitions, theoretical constructs, and empirical findings were extracted and logged in a traceable evidence table (refer to an internal evidence log, not publicly available in this context, but reflecting the process outlined in the constitution). Secondly, a comparative study was performed on the identified agentic AI frameworks (e.g., CrewAI, LangGraph, AutoGen) to delineate their architectural principles, communication protocols, memory management strategies, and safety mechanisms (Derouiche et al., 2025). This comparative lens facilitated the identification of best practices and common limitations. Thirdly, a critical synthesis was undertaken to integrate these findings, evaluate the trustworthiness and verifiability of agent-generated artifacts, and interpret the implications for human-agent collaboration and the future of software engineering. All claims derived from the analysis were rigorously validated against their primary sources to ensure accuracy and reproducibility.

### 3.4. Limitations of Methodology

Despite the systematic approach, this methodology has certain limitations. The reliance on published literature means that cutting-edge, unpublished industrial practices or proprietary advancements in agentic AI might not be fully captured. The exclusion of certain grey literature, while maintaining academic rigor, may omit some nascent but relevant developments in fast-moving areas. Furthermore, the conceptual nature of the analysis, while valuable for theoretical framing, does not involve empirical experimentation or case studies of implemented agentic systems, which could provide additional practical insights into their performance and real-world applicability. These limitations are acknowledged to provide a balanced perspective on the scope and generalizability of the research findings.

